### Introduction 
We first introduce the algorithm of the PPO [1] , which is a common algorithm of the Multi-arm bandit solver in reinforcement learning.

![Screen Shot 2023-05-16 at 11.42.51 PM.png](https://intranetproxy.alipay.com/skylark/lark/0/2023/png/158408/1684305808109-b1c385cc-3e7a-43a8-9293-8b1640b52f90.png#clientId=u9d627278-ea4f-4&from=drop&id=IaCwq&originHeight=349&originWidth=776&originalType=binary&ratio=1&rotation=0&showTitle=false&size=84067&status=done&style=none&taskId=ud3e7a682-2e19-4b9c-92a5-c733b59907c&title=)

### Implement details 
The algorithm is implemented in the "./test_mini_env/torch_ppo_example.py". We first define the actor-network and critic-network in ActorNetwork class and CriticNetwork class respectively. We follow the network choice used in Rllib and use MLP layers with ReLU activation.<br />  
```python
class ActorNetwork(nn.Module):
    def __init__(self, n_actions, input_dims, lr, fc1_dims=128, fc2_dims=128, device='cpu'):
        super(ActorNetwork, self).__init__()

        self.actor = nn.Sequential(
            nn.Linear(input_dims, fc1_dims),
            nn.ReLU(),
            nn.Linear(fc1_dims, fc2_dims),
            nn.ReLU(),
            nn.Linear(fc2_dims, n_actions),
            nn.Softmax(dim=-1)
        )
        self.optimizer = optim.Adam(self.parameters(), lr=lr)
        self.to(device)

    def forward(self, state):
        dist = self.actor(state)
        dist = Categorical(dist)
        return dist

class CriticNetwork(nn.Module):
    def __init__(self, input_dims, lr, fc1_dims=128, fc2_dims=128, device='cpu'):
        super(CriticNetwork, self).__init__()

        self.critic = nn.Sequential(
            nn.Linear(input_dims, fc1_dims),
            nn.ReLU(),
            nn.Linear(fc1_dims, fc2_dims),
            nn.ReLU(),
            nn.Linear(fc2_dims, 1)
        )

        self.optimizer = optim.Adam(self.parameters(), lr=lr)
        self.to(device)

    def forward(self, state):
        value = self.critic(state)
        return value

```

The forward process is run as follows:
```python
def forward(self, obs, state=None, info={}):
    # one hot enconding
    state = self.obs_encode(obs)

    dist = self.actor(state)
    value = self.critic(state).squeeze(-1)
    action = dist.sample()

    logits = dist.log_prob(action)
    return logits, value, action, dist
```
Here the observation is first encoded with one-hot embedding. The actions' estimated winning probabilities (Q-values) are generated by the actor-network and the value function is approximated by the critic-network. Finally, the action for the observation is sampled with estimated winning probabilities.

The loss function is constructed as follows:
```python

def loss_function_construction(reward_batch,vals_batch,prob_ratio,max_value,cfr_reward_batch = None,policy_clip = 0.2):
    # estimate advantage with rewards and estimated values 
    advantage = reward_batch - vals_batch
    
    # compute the clipped winning probabilities
    weighted_probs = advantage * prob_ratio
    weighted_clipped_probs = torch.clamp(prob_ratio, 1 - policy_clip, 1 + policy_clip) * advantage
    # compute the actor loss
    actor_loss = torch.mean(-torch.min(weighted_probs, weighted_clipped_probs))
    # The critic can be computed with CFR loss adaptively
    if cfr_reward_batch is not None:
        critic_loss = torch.mean((cfr_reward_batch - max_value) ** 2)  
    else:
        critic_loss = torch.mean((reward_batch - max_value) ** 2)  
    return actor_loss + critic_loss / 2

```
### Specific examples of PPO
We provide a example of how to apply provided algorithm into agent policy decision making.<br />We take "simple_greedy_agent" to become a smart agent as an example, where the detailed definition can be found in the "./agent/custom_agent.py"<br />When the action generator method is called, we apply epsilon greedy algorithm through the "self.algorithem.generate_action"
```python
def generate_action(self, obs):
    encoded_action = self.algorithm.generate_action(obs=self.get_latest_true_value())

    # Decode the encoded action 
    action = encoded_action % (self.algorithm.bidding_range)

    return action
```



---

### Reference
[1] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. (2017). Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_.
